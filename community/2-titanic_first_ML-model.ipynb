{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your first machine learning model\n",
    "\n",
    "With the Exploratory Data Analysis (EDA) and the baseline model at hand, you can start working on your first, real Machine Learning model. Before you get started, import all necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Figures inline and set visualization style\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# Import data\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below, you will drop the target 'Survived' from the training dataset and create a new DataFrame `data` that consists of training and test sets combined;\n",
    "* But first, you'll store the target variable of the training data for safe keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store target variable of training data in a safe place\n",
    "survived_train = df_train.Survived\n",
    "\n",
    "# Concatenate training and test sets\n",
    "data = pd.concat([df_train.drop(['Survived'], axis=1), df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check out your new DataFrame `data` using the `info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    1309 non-null int64\n",
      "Pclass         1309 non-null int64\n",
      "Name           1309 non-null object\n",
      "Sex            1309 non-null object\n",
      "Age            1046 non-null float64\n",
      "SibSp          1309 non-null int64\n",
      "Parch          1309 non-null int64\n",
      "Ticket         1309 non-null object\n",
      "Fare           1308 non-null float64\n",
      "Cabin          295 non-null object\n",
      "Embarked       1307 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 122.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 numerical variables that have missing values. \n",
    "\n",
    "What are they?\n",
    "\n",
    "That's right, you're missing values for the `'Age'` and `'Fare'` columns! You see in the result of the `.info()` method above that you're missing 263 values for the first column, while you're only missing one for the second column. Also notice that `'Cabin'` and `'Embarked'` are also missing values and you'll need to deal with that also at some point.\n",
    "\n",
    "However, now you'll focus on fixing the numerical variables: you will impute or fill in the missing values for the `'Age'` and `'Fare'` columns, using the median of the of these variables where we know them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    1309 non-null int64\n",
      "Pclass         1309 non-null int64\n",
      "Name           1309 non-null object\n",
      "Sex            1309 non-null object\n",
      "Age            1309 non-null float64\n",
      "SibSp          1309 non-null int64\n",
      "Parch          1309 non-null int64\n",
      "Ticket         1309 non-null object\n",
      "Fare           1309 non-null float64\n",
      "Cabin          295 non-null object\n",
      "Embarked       1307 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 122.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Impute missing numerical variables\n",
    "data['Age'] = data.Age.fillna(data.Age.median())\n",
    "data['Fare'] = data.Fare.fillna(data.Fare.median())\n",
    "\n",
    "# Check out info of data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That already looks a lot better, right!\n",
    "\n",
    "Also notice that, in the [previous tutorial](https://www.datacamp.com/community/tutorials/kaggle-machine-learning-eda) and [notebook](), you had a bunch of features that you wanted to use to predict because you noticed from your exploratory data analysis that they'll be important for `'Survived'` or not. One of those features was `'Fare'`, but also `'Age'` and `'Sex'` were others!\n",
    "\n",
    "But you want to encode your data with numbers, so you'll want to change 'male' and 'female' to numbers. Use the `pandas` function `.get_dummies()` to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "    Age  SibSp  Parch            Ticket     Fare Cabin Embarked  Sex_male  \n",
       "0  22.0      1      0         A/5 21171   7.2500   NaN        S         1  \n",
       "1  38.0      1      0          PC 17599  71.2833   C85        C         0  \n",
       "2  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S         0  \n",
       "3  35.0      1      0            113803  53.1000  C123        S         0  \n",
       "4  35.0      0      0            373450   8.0500   NaN        S         1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.get_dummies()` allows you to create a new column for each of the options in `'Sex'`. So it creates a new column for `female`, called `'Sex_female'`, and then a new column for `'Sex_male'`, which encodes whether that row was male or female. \n",
    "\n",
    "Now, because you added the `drop_first` argument in the line of code above, you dropped `'Sex_female'` because, essentially, these new columns, `'Sex_female'` and `'Sex_male'`, encode the same information.\n",
    "\n",
    "So all you have done is create a new column `'Sex_male'`, which has a `1` if that row is a male - and a `0` if that row is female.\n",
    "\n",
    "`.get_dummies()` will be one of your best friends when doing data manipulation for machine learning!\n",
    "\n",
    "* Now, you'll select the columns `['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']` from your DataFrame to build your first machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex_male     Fare   Age  Pclass  SibSp\n",
       "0         1   7.2500  22.0       3      1\n",
       "1         0  71.2833  38.0       1      1\n",
       "2         0   7.9250  26.0       3      0\n",
       "3         0  53.1000  35.0       1      1\n",
       "4         1   8.0500  35.0       3      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns and view head\n",
    "data = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `.info()` to check out `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 5 columns):\n",
      "Sex_male    1309 non-null uint8\n",
      "Fare        1309 non-null float64\n",
      "Age         1309 non-null float64\n",
      "Pclass      1309 non-null int64\n",
      "SibSp       1309 non-null int64\n",
      "dtypes: float64(2), int64(2), uint8(1)\n",
      "memory usage: 52.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the entries are non-null now. Great job!\n",
    "\n",
    "Up until now, you've got your data in a form to build first machine learning model. That means that you can finally build your first machine learning model! \n",
    "\n",
    "Exciting!\n",
    "\n",
    "For more on `pandas`, check out our [Data Manipulation with Python track](https://www.datacamp.com/tracks/data-manipulation-with-python). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a decision tree classsifier? It is a tree that allows you to classify data points, which are also known as predict target variables, based on feature variables. \n",
    "\n",
    "For example, this tree below has a root node that forces you to make a first decision, based on the following question: \"Was `'Sex_male'`\" less than 0.5? In other words, was the data point a female. If the answer to this question is `True`, you can go down to the left and you get `'Survived'`. If `False`, you go down the right and you get `'Dead'`.\n",
    "\n",
    "For now, you don't need to worry about all the additional information that you see in the nodes, such as gini, samples, or value. You can check out all of this later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1514303214/decision_tree_titanic_1_woofvw.pdf\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You first fit such a model to your training data, which means deciding (based on the training data) which decisions will split at each branching point in the tree. For example, that the first branch is on 'Male' or not and that `'Male'` results in a prediction of `'Dead'`. \n",
    "\n",
    "**Note** that it's actually the gini coefficient which is used to make these decisions. At this point, you won't delve deeper into these stuff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before fitting a model to your `data`, split it back into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data.iloc[:891]\n",
    "data_test = data.iloc[891:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You'll use `scikit-learn`, which requires your data as arrays, not DataFrames so transform them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_train.values\n",
    "test = data_test.values\n",
    "y = survived_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you get to build your decision tree classifier! First create such a model with `max_depth=3` and then fit it your data. **Note** that you name your model `clf`, which is short for \"Classifier\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model and fit to data\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature variables `X` is the first argument that you pass to the `.fit()` method, while the target variable, `y`, is the second argument.\n",
    "\n",
    "The output tells you all that you need to know about your Decision Tree Classifier that you just built: as such, you see, for example, that the max depth is set at 3.\n",
    "\n",
    "* Now, you'll make predictions on your test set, create a new column `'Survived'` and store your predictions in it. Save 'PassengerId' and 'Survived' columns of `df_test` to a .csv and submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions and store in 'Survived' column of df_test\n",
    "Y_pred = clf.predict(test)\n",
    "df_test['Survived'] = Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test[['PassengerId', 'Survived']].to_csv('data/predictions/1st_dec_tree.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the accuracy of your model, as reported by Kaggle?\n",
    "\n",
    "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1514304185/Screen_Shot_2017-12-26_at_16.46.20_i5azys.png\"/>\n",
    "\n",
    "The accuracy is 78%. You have advanced over 2,000 places!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you've got your data in a form to build first machine learning model. On top of that, you've also built your first machine learning model: a decision tree classifier.\n",
    "\n",
    "Now, you'll figure out what this `max_depth` argument was, why you chose it and explore `train_test_split`.\n",
    "\n",
    "For more on `scikit-learn`, check out our [Supervised Learning with scikit-learn course](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is a Decision Tree Classifier?\n",
    "\n",
    "The Decision Tree Classifier you just built had a `max_depth=3` and it looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1514303215/decision_tree_titanic_3_qfmmmu.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximal distance between the first decision and the last is 3, so that's `max_depth=3`. \n",
    "\n",
    "**Note**: you can use `graphviz` to generate figures such as this. See the `scikit-learn` documentation [here](http://scikit-learn.org/stable/modules/tree.html) for further details. \n",
    "\n",
    "In building this model, what you're essentially doing is creating a decision boundary in the space of feature variables, for example (image from [here](http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1514303214/dec_bound_fenxhu.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Choose `max_depth=3` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The depth of the tree is known as a hyperparameter, which means a parameter you need to decide before you fit the model to the data. If you choose a larger `max_depth`, you'll get a more complex decision boundary. \n",
    "\n",
    "* If your decision boundary is too complex, you can overfit to the data, which means that your model will be describing noise as well as signal.\n",
    "\n",
    "* If your `max_depth` is too small, you might be underfitting the data, meaning that your model doesn't contain enough of the signal.\n",
    "\n",
    "But how do you tell whether you're overfitting or underfitting?\n",
    "\n",
    "**Note**: this is also referred to as the bias-variance trade-off and you won't go into details on that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way is to hold out a test set from your training data. You can then fit the model to your training data, make predictions on your test set and see how well your prediction does on the test set. \n",
    "\n",
    "* You'll now do this: split your original training data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, you'll iterate over values of `max_depth` ranging from `1` to `9` and plot the accuracy of the models on training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "dep = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(dep))\n",
    "test_accuracy = np.empty(len(dep))\n",
    "\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(dep):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=k)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = clf.score(X_train, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = clf.score(X_test, y_test)\n",
    "\n",
    "# Generate plot\n",
    "plt.title('clf: Varying depth of tree')\n",
    "plt.plot(dep, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(dep, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1514303706/download_jlzbsx.png\"/>\n",
    "\n",
    "As you increase the `max_depth`, you're going to fit better and better to the training data because you'll make decisions that describe the training data. The accuracy for the training data will go up and up, but you see that this doesn't happen for the test data: you're overfitting.\n",
    "\n",
    "So that's why you chose `max_depth=3`. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, you've got your data in a form to build first machine learning model. Nex,t you've built also your first machine learning model: a decision tree classifier. Lastly, you learned about `train_test_split` and how it helps us to choose ML model hyperparameters.\n",
    "\n",
    "In the next tutorial, which will appear on the DataCamp Community on the 3rd of January 2018, you'll learn how to engineer some new features and build some new models! \n",
    "\n",
    "In the meantime, if you want to discover more on `scikit-learn`, check out DataCamp's [Supervised Learning with scikit-learn course](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
